---
title: "Post: Google Summer of Code 2022 - CERN-HSF"
categories:
  - Blog
tags:
  - gsoc
  - projects
---

# GSoC 2022 CERN-HSF
## Optimization of GPU Tracking Pipeline for ACTS GPU R&D

This post discusses my experiences, learnings and achieved results over the past 4 months as a contributor to ACTS under CERN. Relavent links to code and benchmarks are provided along the way.

## Introduction
Acts is a track reconstruction software toolkit for high energy physics experiments. With the potentially increased number of particle interactions in the High Luminosity Large Hadron Collider (HL-LHC) experiments in the future track reconstruction time will also increase. Therefore, Acts GPU R&D (Research and Development) is conducted under traccc, vecmem and detray to accelerate the track reconstruction time. Vecmem provides memory management tools for convenient GPU memory management and caching allocators, Detray is a geometry builder which translates the CPU geometry into GPU one (did not get my head around this one yet.) and finally Traccc demonstrates the GPU tracking pipeline.

My focus is on improving the throughput of the Traccc pipeline and bench-marking the results. This is achieved by using caching allocators provided by Vecmem and CUDA-MPS or CUDA-MIG which are two ways to improve concurrent GPU utilization.

### Lets understand the use case briefly

Particles colliding at very high speeds break up into other (smaller) particles and are projected outwards. These projected particles hit very sensitive detector planes, these interactions between the particles and the detection planes (known as cells) are recorded and ultimately used to generate the particle track. In reality many such particles are produced and they may interact with each (like a chain reaction) producing a very large number of events. Therefore improving the throughput (events/time) is important for algorithms.

## How can CUDA Multi-Process Service (MPS) Improve Throughput?

Nvidia documentation explains this a lot clearly than I can, but let me provide the necessary information briefly.

Nvidia uses concurrent scheduling, which means gets scheduled at the same time if possible, to schedule kernels from work queues of the same process (same CUDA context). However, it uses time sliced scheduling to schedule kernels from multiple processes (different CUDA contexts). Therefore multiple processes cannot utilize the GPU simultaneously.

To put it in simple terms (maybe slightly inaccurate though ðŸ˜‰), the multi-process server acts as a middle man receiving kernels from multiple processes and submitting these kernels to the GPU as if it were from the same CUDA context.

Imagine a program occupying just 10% of your GPUâ€™s resources, theoretically you can run 10 such processes on your GPU simultaneously with MPS to occupy ~100% of your GPU. This is the main advantage of using MPS.

Image to mps here

Good to know about Nvidiaâ€™s HyperQ (available on post Kepler architectures) which allows submitting GPU kernels to the GPU simultaneously. Imagine a single lane highway from the host to device versus a 32 lane highway (here highways meaning workload queues).

MPS documentation : https://docs.nvidia.com/deploy/mps/index.html

## Traccc Pipeline

This entire pipeline can be broken down into sub components as follows

Clusterization - grouping cells that are adjacent to each other.
Measurement Creation - calculates the weighted averages of each cluster positions.
Spacepoint Formation - converts these measurementâ€™s positions from local. to global space
Seeding - finds out a set of 3 spacepoints that belong to the same bin.
Track parameter estimation - does global to local transform.

Image to pipeline here

### Definitions that will be helpful to understand the pipeline

Cells - are particle interactions on the detector plane. It contains information about the local position on the detector plane. A single particle interaction produces multiple cells.
Clusters - are a group of cells that are adjacent to each other in the detector plane.
Measurements - are weighted averaged local positions of the cells in a cluster.
Spacepoints - are local positions of measurements transformed onto global positions. This will be in input to the seeding algorithm.
Binning - groups spacepoints to a 2D grid. A bin is a section of detector geometry which contains a number of spacepoints.
Seeds - are a set of three spacepoints may belong in the same or adjacent bins and might be produced by the same particle.
Prototracks - produced by track parameter estimation is a global to local transformation on the surface.
Track - is the set of all spacepoints produced by the same particle.

https://github.com/acts-project/traccc


## Okay.. so how does MPS help Traccc?

Generally a single instance of Traccc computes several events sequentially and each event undergoes pipeline shown in Figure-01.
Shown below is how multiple processes would execute 10 events each.

Image to parallel processes

The algorithms in the Traccc pipeline are parallelized using CUDA and each event do not fully occupy the GPU resources. Moreover, collision events are independent to each other, hence multiple processes can run simultaneously on different event data improving the overall throughput.

